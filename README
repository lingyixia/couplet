Usage:
 4个python文件只需要动Main.py即可
 108行和106行选一取消注释用以train
 108行是边train边eval,但是eval会减慢总体速度,可以开始先用这个看看效果随后注释掉108，使用106行train即可(其实机器好直接用108行就行了，主要是比较穷。。。)
 108行只train无eval
 train一段时间后把108和106都注释吊即可,112行是predict，重新运行即可看模型predict结果
other:
 每120秒会保存一个模型，最多保存5个

对联生成，优化中
2019.6.28 添加attention
2019.7.2 添加beamsearch
2019.7.10 发现将输出层Dense的激活函数去掉收敛速度会大大提升，还不清楚原因
 - 似乎是想明白了，倒数第二层我开始用的relu激活函数，relu之后有些神经元就死了，虽然经过softmax会有一些缓解但是这些死的神经元更新依旧很慢，因此收敛很慢，结果很受初始化参数的影响(eg:"春风风去一人人"))
 因此relu虽然好，但是坚决不能用在后面，总之要用就用到前几层，离输出层远点，这样即使死了也能在后面层中复活，达到加速收敛的目的。
2019.7.11 将decoder的多层lstm改为单个，用以减少参数，发现效果并没减。
下一个目标:加平仄

效果:
上联:天增岁月人增寿 
下联:春满乾坤福满门
其他
春满乾坤福满堂
春满乾坤岁更新
春满乾坤地生辉
春满乾坤景焕新

上联:欲穷千里目 
下联:更上一层楼
其他
不负万年心
不负一枝春
不负一生心
不负一生情

上联:春风又绿江南岸 
下联:旭日初红塞北天
其他
旭日初升塞北天
明月初圆塞北天
旭日初红塞北花
明月常明塞北天

目前训练20万轮作用，batchsize=32，loss=3.2左右，但是还处于下降的趋势，继续训练效果肯定会更好，但是人穷没办法，使用的google colab，虽然比本机块个几倍但是还是不够快，先这样吧
试用:http://49.232.34.153:7777/ 界面很丑勿喷,也没处理异常,只用于测试
