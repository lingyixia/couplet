Usage:
 4个python文件只需要动Main.py即可
 105行和106行选一取消注释用以train
 105行是边train边eval,但是eval会减慢总体速度,可以开始先用这个看看效果随后注释掉105，使用106行train即可
 106行只train无eval
 train一段时间后把105和106都注释吊即可,112行是predict，重新运行即可看模型predict


对联生成，优化中
2019.6.28 添加attention
2019.7.2 添加beamsearch
2019.7.10 发现将输出层Dense的激活函数去掉收敛速度会大大提升，还不清楚原因
 - 似乎是想明白了，倒数第二层我开始用的relu激活函数，relu之后有些神经元就死了，虽然经过softmax会有一些缓解但是这些死的神经元更新依旧很慢，因此收敛很慢，结果很受初始化参数的影响(eg:"春风风去一人人"))
 因此relu虽然好，但是坚决不能用在后面，总之要用就用到前几层，离输出层远点，这样即使死了也能在后面层中复活，达到加速收敛的目的。
2019.7.11 将decoder的多层lstm改为单个，用以减少参数，发现效果并没减。
下一个目标:加平仄

效果:
上联:天增岁月人增寿
下联:春满乾坤景更新
其他
春满乾坤景焕新
春满乾坤地纳祥
人沐春风我爱民
春满乾坤地纳财

目前训练20万轮作用，batchsize=32，loss=3.2左右，但是还处于下降的趋势，继续训练效果肯定会更好，但是人穷没办法，使用的google colab，虽然比本机块个几倍但是还是不够快，先这样吧
试用:http://49.232.34.153:7777/ 界面很丑勿喷,只用于测试
